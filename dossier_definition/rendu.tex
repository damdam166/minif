\documentclass{article}
\usepackage[frenchb]{babel}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amssymb}


\begin{document}


\title{Compte rendu tp 1 - Théorie de l'information}
\author{DELPY Damien, DE GENDRE Alexandre , BILLON Erwan}
\maketitle

% To display the number of the page
\pagestyle{plain}


\newpage
\section{Lien entre entropie discrète et continue}
	
\vspace{2\baselineskip}

	\textbf{Question 1}

\vspace{2\baselineskip}

Comme $f_X$ est continue sur $\mathbb{R}$, d'après le théorème fondamental de l'intégration, la fonction
F : x $\mapsto$ $\int_0^{x} f_X(t)dt$ est $C^1$ sur $\mathbb{R}$ et de dérivée $f_X$.

\vspace{2\baselineskip}

On applique le théorème des accroissements finis à F pour a = i$\Delta$ et b = (i + 1)$\Delta$ :

- F est continue sur le segment [a, b] avec $a < b$

- F est dérivable sur l'ouvert ]a, b[

Ainsi il existe $x_i$ $\in$ ]a, b[ tel que : 

$$ F'(x_i) = \frac{F(b) - F(a)}{b - a} $$

\vspace{2\baselineskip}

Ainsi par relation de Chasles sur le segment, et comme F$'$($x_i$) = $f_X(x_i)$,

$$ f_X(x_i) = \frac{\int_{a}^{b} f_X(x)dx}{\Delta} $$

\vspace{2\baselineskip}

	\textbf{Question 2}

\vspace{2\baselineskip}

Commençons par chercher $X_{\Delta}\{\Omega\}$.

On a : 

\vspace{2\baselineskip}

$X_{\Delta}\{\Omega\}$ = $\{X_{\Delta}(\omega) / \omega \in \Omega\}$ = $\{ x_i / i \in \mathbb{Z} \}$.

\vspace{2\baselineskip}

Soit x $\in$ $X_{\Delta}$($\Omega$), déterminons $\mathbb{P}$($X_{\Delta}$ = x).

Il existe i $\in$ $\mathbb{Z}$ tel que x = $x_i$

On a,

$$ \mathbb{P}(X_{\Delta} = x) = \mathbb{P}(1_{[k\Delta, (k+1)\Delta]}(X) = 1) $$

\newpage

En effet, d'après la question précédente, il n'existe pas j $\in$ $\mathbb{Z}$ tel que x = j$\Delta$, ce qui nous assure que x ne puisse pas être dans deux segments à la fois.

\vspace{2\baselineskip}

Ainsi, on a : 

$$ \mathbb{P}(X_{\Delta} = x) = \mathbb{P}(X \in [k\Delta, (k+1)\Delta]) $$

\vspace{2\baselineskip}

Comme X est une variable aléatoire à densité, on trouve que :

$$ \mathbb{P}(X_{\Delta} = x) = \int_{k\Delta}^{(k+1)\Delta} f_X(t)dt $$

\vspace{2\baselineskip}

\underline{Conclusion}: d'après la question précédente, pour tout x $\in$ $X_{\Delta}(\Omega) = \{x_i / i \in \mathbb{Z}\} $, on a : 

$$ \mathbb{P}(X_{\Delta} = x) = \Delta f_X(x_k) $$ avec k $\in$ $\mathbb{Z}$

\vspace{2\baselineskip}

	\textbf{Question 3}

\vspace{2\baselineskip}

Par définition de l'entropie on a,

$$ \mathbb{H}(X_{\Delta}) = - \sum_{i\in\mathbb{Z}}^{} \mathbb{P}(X_{\Delta} = x_i)log(\mathbb{P}(X_{\Delta}) = x_i)) $$

\vspace{2\baselineskip}

Ainsi d'après la question précédente, en remplaçant,

$$ \mathbb{H}(X_{\Delta}) = - \Delta \sum_{i\in\mathbb{Z}} f_X(x_i)( log(f_X(x_i)) + log(\Delta) ) $$

\vspace{\baselineskip}

Ainsi, pour des sommes finies,  pour tout (i,j) $\in$ $\mathbb{Z}x\mathbb{Z}$, 

$$ - \Delta \sum_{k=i}^{j} f_X(x_k)( log(f_X(x_k)) + log(\Delta) ) = - \Delta \sum_{k=i}^{j} f_X(x_k)log(f_X(x_k)) - \Delta \sum_{k=i}^{j} f_X(x_k) log(\Delta)  $$

\vspace{2\baselineskip}

On peut passer à la limite sur i vers -$\infty$ et j vers +$\infty$ car on reconnait une somme valant 1.

\vspace{2\baselineskip}

En effet, 

$$ \int_{-\infty}^{+\infty} f_X(t)dt = 1 $$ par définition d'une densité.

\vspace{2\baselineskip}

	\textbf{Question 4}

\vspace{2\baselineskip}

On suppose que le résultat est vérifié.

\vspace{2\baselineskip}

La différence entropique entre le cas continu et le cas discret est chiffrée par l'apparition d'un pas $\Delta$.

\newpage
\section{Loi Gaussienne}
	\subsection{Loi gaussienne univariée}
		\textbf{Question 1}

% To jump 2 lines
\vspace{2\baselineskip}

La génération de 10000 réalisations d'une variable aléatoire X suivant une loi gaussienne N($\mu$, $\sigma^2$)
			avec $\mu$ = 2 et $\sigma^2$ = 9 écrit en Python en utilisant numpy.random.randn est en \textbf{figure 1}. 

% To jump 2 lines
\vspace{2\baselineskip}

		\textbf{Question 2}

\vspace{2\baselineskip}

Après avoir tracé l'histogramme avec le code fourni, on obtient la \textbf{figure 2}.

L'autre courbe est celle de la densité $f_X$.

\vspace{2\baselineskip}

\begin{figure}[h]
	% Left side
	\begin{minipage}[t]{0.45\textwidth}
		\centering
        	\includegraphics[width=0.9\textwidth]{pics/part_2_q_1.png}
        	\caption{Code source python sur la loi normale.}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.45\textwidth} % Right side
		\centering
		\includegraphics[width=0.9\textwidth]{pics/plot_1.png}
		\caption{Histogramme de la loi normale N(2,9).}
	\end{minipage}
\end{figure}
	
\vspace{2\baselineskip}

L'idée pour tracer $f_X$ est de subdiviser les valeurs d'entrées X avec np.linspace(min, max, pas), entre np.min(X) et np.max(X).

\vspace{1\baselineskip}

Le pas a été choisi à 100 ici.

\vspace{2\baselineskip}

L'histogramme représente une loi gaussienne avec des valeurs discrètes.

Il ressemble donc naturellement à une intégrale de Riemann, une approximation de la densité.

\newpage
		\textbf{Question 3}

\vspace{2\baselineskip}

Pour l'entropie \textbf{numérique}, l'idée est de faire une approximation de la valeur H(X)
à l'aide du théorème de convergence sur les sommes de Riemann.

\vspace{1\baselineskip}

La fonction g = $f_Xlog(f_X)$ est continue (par morceaux) sur $\mathbb{R}$.

Ainsi pour tout segment [a,b] de réels avec $a < b$, la suite

$$ (\frac{1}{b-a} \sum_{i=1}^{n} g(a + i\frac{b-a}{n}) )_n $$

converge vers l'intégrale 

$$ \frac{1}{b-a} \int_{a}^{b} g(x)dx $$

\vspace{2\baselineskip}

Pour l'entropie \textbf{théorique}, elle est déterminable par le calcul car X suit une loi normale univariée :

$$ \mathbb{H}(X) = log_2(\sigma\sqrt{e2\pi}) $$


\vspace{2\baselineskip}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{pics/fig_entropy.png}
	\caption{Code source de l'entropie numérique et théorique.}
\end{figure}

\vspace{2\baselineskip}

\newpage
	\subsection{Loi gaussienne multivariée}
		\textbf{Question 1}

\vspace{2\baselineskip}

Pour générer X à l'aide de randn, il suffit d'utiliser des vecteurs au lieu de scalaires,
la fonction numpy.random.randn fonctionne aussi dans ce cas, voir \textbf{figure 4}.
	
\vspace{2\baselineskip}

		\textbf{Question 2}

\vspace{2\baselineskip}

Voir \textbf{figure 4 et 5}.

\vspace{2\baselineskip}

		\textbf{Question 3}

\vspace{2\baselineskip}

Pour afficher les lignes de niveau, il est possible d'utiliser plt.scatter en faisant du slicing sur
le vecteur X, voir \textbf{figure 6}.

\vspace{2\baselineskip}

Sur la \textbf{figure 6}, \\c = 1 en jaune; \\ c = $\sqrt{5}$ en rouge; \\ c = $\sqrt{10}$ en vert.

\vspace{2\baselineskip}

Les lignes de niveau représentent des courbes d'équiprobabilité dans l'espace des variables aléatoires, et leur orientation et leur forme dépendent de la matrice de covariance R.


\vspace{2\baselineskip}

\begin{figure}[h]
	\begin{minipage}[t]{0.30\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{pics/generate_multi_law.png}
		\caption{Code source, loi multivariée}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{pics/plot_2.png}
		\caption{Loi gaussienne multivariée sur $\mathbb{R}^2$}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.30\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{pics/plot_3.png}
		\caption{Lignes de niveaux}
	\end{minipage}
\end{figure}

\newpage
\section{Analyse de données}
	\textbf{Question 1} OK

\vspace{2\baselineskip}

	\textbf{Question 2} 

\vspace{2\baselineskip}

Les estimateurs empiriques sont calculés grâce à numpy.mean et numpy.cov, voir \textbf{figure 7}.

Pour calculer la loi gaussienne multivariée, il suffit d'utiliser la fonction numpy.random.multivariate\_normal.

\vspace{2\baselineskip}

Les données de \textbf{Bordeaux/Santiago} occupent le plus d'espace, la variance de ce cluster est grande.

Les données de \textbf{Bordeaux/Nantes} occupent le moins d'espace, la variance de ce cluster est petite.

Cela est logique car Bordeaux et Nantes ont presque le même climat, alors que Bordeaux et Santiago ou Nantes et Santiago non.

La modélisation par la loi gaussienne multivariée remplit bien son rôle.

\vspace{2\baselineskip}

	\textbf{Question 3}

\vspace{2\baselineskip}

	\textbf{Question 4}

\vspace{2\baselineskip}

\begin{figure}[h]
	\begin{minipage}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{pics/code_plot_4.png}
		\caption{Code source associé aux estimateurs empiriques}
	\end{minipage}
	\hfill %
	\begin{minipage}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{pics/plot_4.png}
		\caption{La loi gaussienne multivariée sur les données pluviométriques}
	\end{minipage}
\end{figure}

\end{document}
